# SIMPLE AI TRANSLATOR USING ðŸ¦œðŸ”—
This is the code for a simple AI translation web app that i created to practice the use of Langchain and Ollama. In order to run it locally, first setup your Ollama environment and download a model. You can see more details on how to do so here: [Ollama](https://ollama.com/). Also download the required dependencies by running `pip install -r requirements.txt `, preferably in a virtual environment. Then, in the project folder, setup a .env file with the following environment variables: `OLLAMA_HOST = "(optional)" , INFERENCE_MODEL = "mistral" (choose the model here, mistral is the default)`. In case you are not running Ollama locally, you can provide the IP address where its running in the OLLAMA_HOST environment variable. After doing so, run the app.py file with `streamlit run app.py` and you should see the translator running in localhost:8000